{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tools.data_setter import cifar_100_setter\n",
    "from models import cifar, imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.getcwd()\n",
    "dataset = 'cifar10'\n",
    "teacher = 'wrn-28-4'\n",
    "student = 'wrn-16-2'\n",
    "\n",
    "result_dirname = os.path.join(dirname, 'results', dataset, teacher, student)\n",
    "\n",
    "t_list = [\"1.0\", \"3.0\", \"5.0\", \"20.0\", \"inf\"]\n",
    "alpha_list = [\"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\", \"0.6\", \"0.7\", \"0.8\", \"0.9\", \"1.0\"]\n",
    "accuracy_list = np.zeros([len(t_list), len(alpha_list)])\n",
    "\n",
    "t_list = [\"20.0\"]\n",
    "alpha_list = [\"1.0\"]\n",
    "\n",
    "for t_enum, t in enumerate(t_list):\n",
    "    for alpha_enum, alpha in enumerate(alpha_list):\n",
    "        accuracy = []\n",
    "        current_dirname = os.path.join(result_dirname, 'alp_{}_T_{}'.format(alpha, t))\n",
    "        current_files = os.listdir(current_dirname)\n",
    "        \n",
    "        current_files = [f for f in current_files if 'csv' in f]\n",
    "        current_files = [f for f in current_files if f.split('_')[2]==\"1.0\" and f.split('_')[5]==\"1.0\"]\n",
    "        for file in current_files:\n",
    "            accuracy.append(max(list(pd.read_csv(os.path.join(current_dirname, file))['train_accuracy'])))\n",
    "    accuracy_list[t_enum, alpha_enum] = np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.pcolor(accuracy_list, cmap=plt.get_cmap('Blues'))\n",
    "plt.xticks(np.arange(0.5, len(alpha_list), 1), alpha_list)\n",
    "plt.yticks(np.arange(0.5, len(t_list), 1), t_list)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Temperature')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "# plt.savefig('train_accuracy.pdf', bbox_inches='tight', format='pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy heatmap & 4 cases analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_list(dataloaders, dataloaders_mode, alpha=None, t=None, mode=None):\n",
    "    device = torch.device('cuda:1')\n",
    "\n",
    "    teacher = cifar.WideResNet(depth=28, widen_factor=4, num_classes=100)\n",
    "    filename = './model_checkpoints/cifar100/None/wrn-28-4/alp_0.1_T_1.0/random_highest_1.0_random_highest_1.0_seed9999.t1'\n",
    "    checkpoint = torch.load(filename, map_location=device)['199']\n",
    "    teacher.load_state_dict(checkpoint, strict=True)\n",
    "    \n",
    "    student = cifar.WideResNet(depth=16, widen_factor=2, num_classes=100)\n",
    "    filename = './model_checkpoints/cifar100/wrn-28-4/wrn-16-2/alp_{}_T_{}/random_highest_1.0_random_highest_1.0_seed9999.t1'.format(alpha, t)\n",
    "    checkpoint = torch.load(filename, map_location=device)['199']\n",
    "    student.load_state_dict(checkpoint, strict=True)\n",
    "    \n",
    "    teacher.eval().to(device)\n",
    "    student.eval().to(device)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    entropy_list = []\n",
    "    student_labels = []\n",
    "    \n",
    "    for i, data in enumerate(dataloaders[dataloaders_mode]):\n",
    "        image = data[0].type(torch.FloatTensor).to(device)\n",
    "        label = data[1].type(torch.LongTensor).to(device)\n",
    "        \n",
    "        teacher_label = teacher(image)\n",
    "        student_label = student(image)\n",
    "        \n",
    "        teacher_prob = torch.softmax(teacher_label, dim=1)\n",
    "        entropy = torch.sum(-teacher_prob*torch.log(teacher_prob), dim=1)\n",
    "        \n",
    "        student_label = torch.max(student_label, dim=1)[1]\n",
    "    \n",
    "        labels += label.tolist()\n",
    "        entropy_list += entropy.tolist()\n",
    "        student_labels += student_label.tolist()\n",
    "\n",
    "    return labels, entropy_list, student_labels\n",
    "\n",
    "def check_tf_entropy(labels, entropy_list, student_labels):\n",
    "    gt = (np.array(student_labels)==np.array(labels)).tolist()\n",
    "    \n",
    "    tf_dict = {}\n",
    "    keys = ['t', 'f']\n",
    "    for key in keys:\n",
    "        tf_dict[key] = []\n",
    "\n",
    "    for idx, gt_ in enumerate(gt):\n",
    "        key = str(gt_).lower()[0]\n",
    "        tf_dict[key].append(idx)\n",
    "        \n",
    "    selected_entropy_dict = {}\n",
    "    for k, v in tf_dict.items():\n",
    "        selected_entropy_list = []\n",
    "        for idx in v:\n",
    "            selected_entropy_list.append(entropy_list[idx])\n",
    "        selected_entropy_dict[k] = np.mean(selected_entropy_list)\n",
    "        \n",
    "    return selected_entropy_dict['t'], selected_entropy_dict['f']\n",
    "\n",
    "def check_changed_index(labels, student1_labels, student2_labels):\n",
    "    changed_index_dict = {}\n",
    "    keys = ['t->t', 't->f', 'f->t', 'f->f']\n",
    "    for key in keys:\n",
    "        changed_index_dict[key] = []\n",
    "        \n",
    "    gt1 = (np.array(student1_labels)==np.array(labels)).tolist()\n",
    "    gt2 = (np.array(student2_labels)==np.array(labels)).tolist()\n",
    "    \n",
    "    for idx, (gt1_, gt2_) in enumerate(zip(gt1, gt2)):\n",
    "        pre = str(gt1_).lower()[0]\n",
    "        post = str(gt2_).lower()[0]\n",
    "        key = pre + '->' + post\n",
    "        changed_index_dict[key].append(idx)\n",
    "        \n",
    "    return changed_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset fix\n",
    "teacher = cifar.WideResNet(depth=28, widen_factor=4, num_classes=100)\n",
    "dataloaders, dataset_size = cifar_100_setter(teacher=teacher,\n",
    "                                             mode='crop',\n",
    "                                             batch_size=128,\n",
    "                                             root='/home/osilab7/hdd/cifar',\n",
    "                                             model_name='cifar100/wrn-28-4/wrn-16-2/alp_1.0_T_20.0/random_highest_1.0_random_highest_1.0_seed9999.t1',\n",
    "                                             cls_acq='random',\n",
    "                                             cls_order='highest',\n",
    "                                             zeta=1.0,\n",
    "                                             sample_acq='random',\n",
    "                                             sample_order='highest',\n",
    "                                             delta=1.0)\n",
    "\n",
    "for i, (image, target) in enumerate(dataloaders['train']):\n",
    "    if i == 0:\n",
    "        images = image.cpu()\n",
    "        targets = target.cpu()\n",
    "    else:\n",
    "        images = torch.cat([images, image.cpu()], dim=0)\n",
    "        targets = torch.cat([targets, target.cpu()], dim=0)\n",
    "        \n",
    "dataloaders['train'] = torch.utils.data.DataLoader(list(zip(images, targets)), batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_mode = \"train\"\n",
    "t_list = [\"1.0\", \"3.0\", \"5.0\", \"20.0\", \"inf\"]\n",
    "alpha_list = [\"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\", \"0.6\", \"0.7\", \"0.8\", \"0.9\", \"1.0\"]\n",
    "true_entropy_list = np.zeros([len(t_list), len(alpha_list)])\n",
    "false_entropy_list = np.zeros([len(t_list), len(alpha_list)])\n",
    "\n",
    "for t_enum, t in enumerate(t_list):\n",
    "    for alpha_enum, alpha in enumerate(alpha_list):\n",
    "        labels, entropy_list, student_labels = get_entropy_list(dataloaders, dataloaders_mode, alpha, t)\n",
    "        true_entropy, false_entropy = check_tf_entropy(labels, entropy_list, student_labels)\n",
    "        true_entropy_list[t_enum, alpha_enum] = true_entropy\n",
    "        false_entropy_list[t_enum, alpha_enum] = false_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.pcolor(true_entropy_list, cmap=plt.get_cmap('Blues'))\n",
    "plt.xticks(np.arange(0.5, len(alpha_list), 1), alpha_list)\n",
    "plt.yticks(np.arange(0.5, len(t_list), 1), t_list)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Temperature')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "# plt.savefig('train_entropy.pdf', bbox_inches='tight', format='pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
